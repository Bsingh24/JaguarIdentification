{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "478b4e44-6514-4877-ac34-4d1976ca7dae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Resnet import ResNet, Block, create_pairs\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "from torch.nn import TripletMarginLoss\n",
    "from torchvision.io import decode_image, ImageReadMode\n",
    "from torch.utils.data import Dataset, Sampler, DataLoader\n",
    "from torchvision.transforms import v2\n",
    "from torch.optim import Adam\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ed9deb-b889-403c-8573-2b9182dddc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "creates batches with a size of 15, ensuring we are using all samples provided at random.\n",
    "\"\"\"\n",
    "\n",
    "def create_batches(label_mapping, n_classes, n_samples):\n",
    "    all_classes = list(label_mapping.keys())\n",
    "    batches = []\n",
    "\n",
    "    while True:\n",
    "        available_classes = [c for c in all_classes if len(label_mapping[c]) > 0] # Keep only classes if they are still available.\n",
    "        # Exit if we have less\n",
    "        if len(available_classes) < n_classes: \n",
    "            break\n",
    "\n",
    "        # Ensures that we will at least have 1 example of every label pair\n",
    "        selected_classes = random.sample(available_classes, n_classes)\n",
    "        batch = []\n",
    "\n",
    "        for cls in selected_classes:\n",
    "            indices = label_mapping[cls]\n",
    "            # If there are not enough examples to sample from, we re-select a sample from the shortened list. Empty the list after\n",
    "            if len(indices) >= n_samples:\n",
    "                chosen = indices[:n_samples]\n",
    "                label_mapping[cls] = indices[n_samples:]\n",
    "            else:\n",
    "                chosen = indices + random.choices(indices, k=(n_samples - len(indices)))\n",
    "                label_mapping[cls] = []\n",
    "\n",
    "            batch.extend(chosen)\n",
    "\n",
    "        batches.append(batch)\n",
    "\n",
    "    return batches\n",
    "\n",
    "\n",
    "class BalancedBatchSampler(Sampler):\n",
    "    def __init__(self, labels, n_classes, n_samples):\n",
    "        self.labels = labels\n",
    "        self.n_classes = n_classes \n",
    "        self.n_samples = n_samples  \n",
    "\n",
    "        self.label_mapping = defaultdict(list)\n",
    "        for idx, label in enumerate(labels):\n",
    "            self.label_mapping[int(label)].append(idx)\n",
    "\n",
    "    def __iter__(self):\n",
    "        for cls in self.label_mapping:\n",
    "            random.shuffle(self.label_mapping[cls])\n",
    "\n",
    "        batches = create_batches(self.label_mapping, self.n_classes, self.n_samples)\n",
    "        for batch in batches:\n",
    "            yield batch\n",
    "\n",
    "    def __len__(self):\n",
    "        batches = create_batches(self.label_mapping, self.n_classes, self.n_samples)\n",
    "        return len(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4dcce175-22d4-4b83-8268-7fb16e8b3a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, annotation_file, img_dir, transform=None):\n",
    "        self.img_label = pd.read_csv(annotation_file)\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_label)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.img_label.iloc[idx, 0])\n",
    "        image = decode_image(img_path, mode=ImageReadMode.RGB)\n",
    "        label = self.img_label.iloc[idx, 1]\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "57e97a30-89dd-47d3-a244-0e1ca702422e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = v2.Compose([\n",
    "    v2.Resize(256),\n",
    "    v2.CenterCrop(224),\n",
    "    v2.ToDtype(torch.float32, scale=True)])\n",
    "train_data = ImageDataset('Data/train_data.csv', 'Data/train/', transform)\n",
    "valid_data = ImageDataset('Data/valid_data.csv', 'Data/train/', transform)\n",
    "test_data = ImageDataset('Data/test_data.csv', 'Data/train/', transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffb6dd73-0583-47d0-98ab-74d4187395f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "resnet_model = ResNet(Block, [3, 4, 6, 3], image_channels=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a45e6df-ef5c-4e4d-b977-06818cc7f93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/train_data.csv')\n",
    "label_tensor = torch.tensor(df['encoded_ground_truth'].values)\n",
    "\n",
    "sampler = BalancedBatchSampler(label_tensor, 5, 3)\n",
    "\n",
    "dataloader = DataLoader(train_data, batch_sampler=sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "625616b1-5436-4ec6-856b-161235b4213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "epochs = 50\n",
    "batch_size = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b4a9e15-f5fb-41e4-8dc3-e0d4244016c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    x = batch[0]\n",
    "    y = batch[1]\n",
    "    pred = resnet_model(x)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "01045f96-bb70-4145-af14-eb6dadbc441e",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = TripletMarginLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b613a04c-f1bb-41c5-a424-d088ed79ac90",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = create_pairs(y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "06ef8eec-48f4-4970-96eb-f2a6bc3650e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.8518, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "anchor_embedding = []\n",
    "positive_embedding = []\n",
    "negative_embedding = []\n",
    "for anchor, positive, negative in res:\n",
    "    anchor_embedding.append(anchor)\n",
    "    positive_embedding.append(positive)\n",
    "    negative_embedding.append(negative)\n",
    "\n",
    "anchor_tensor = torch.stack(anchor_embedding)\n",
    "positive_tensor = torch.stack(positive_embedding)\n",
    "negative_tensor = torch.stack(negative_embedding)\n",
    "loss(anchor_tensor, positive_tensor, negative_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338a7d5f-ac15-4aba-b50c-365fe4b0239b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
